---
title: "Python Logging Best Practices - Expert Tips with Practical Examples"
slug: "python-logging-best-practices"
date: "2025-07-24"
tags: [python, logging, debugging, monitoring, structured-logging, production, best-practices, observability]
authors: [vivek_goswami]
description: "Master Python logging with 20 proven best practices, code examples, and expert techniques for better debugging, monitoring, and production deployments."
keywords: [Python logging, logging best practices, structured logging, python debugging, logging configuration, production logging, log management, monitoring, observability, SigNoz]
---

Struggling with chaotic logs that hide critical information when you need it most? Python logging doesn't have to be a maze of duplicate messages, performance bottlenecks, and missing debug data.

Most Python developers face these common logging challenges:
- **Log floods** drowning out critical errors in production
- **Performance hits** from expensive logging operations
- **Security vulnerabilities** from accidentally logging sensitive data  
- **Configuration complexity** leading to duplicate or missing messages
- **Poor debugging workflows** that waste hours tracking down issues

This comprehensive guide covers 20 battle-tested Python logging best practices that will transform your debugging experience and production monitoring capabilities.

## Why Python Logging Matters More Than Print Statements

Before diving into best practices, let's establish why proper logging is essential:

```python
# âŒ Problematic debugging approach
def process_payment(amount, user_id):
    print(f"Processing payment: ${amount}")  # No context, can't be disabled
    if amount > 10000:
        print("Large payment detected")  # No severity level
    # Process payment logic
    print("Payment completed")  # No timestamp or source info
```

```python
# âœ… Proper logging approach
import logging

logger = logging.getLogger(__name__)

def process_payment(amount, user_id):
    logger.info("Processing payment", extra={
        "amount": amount, 
        "user_id": user_id,
        "transaction_id": generate_transaction_id()
    })
    
    if amount > 10000:
        logger.warning("Large payment detected", extra={
            "amount": amount,
            "user_id": user_id,
            "flagged_reason": "high_value"
        })
    
    # Process payment logic
    logger.info("Payment processing completed", extra={
        "amount": amount,
        "user_id": user_id,
        "status": "success"
    })
```

The logging approach provides structured data, severity levels, timestamps, and can be centrally configuredâ€”all essential for production applications.

## 1. Understanding Logger Hierarchy and Singletons

Python loggers operate as singletons, meaning `logging.getLogger(name)` always returns the same logger instance for a given name. This design ensures consistency across your application:

```python
import logging

# These are the SAME logger instance
logger1 = logging.getLogger('myapp.database')
logger2 = logging.getLogger('myapp.database')

print(logger1 is logger2)  # True

# Configure once, affects all references
logger1.setLevel(logging.DEBUG)
print(logger2.level)  # 10 (DEBUG level)
```

### Hierarchical Logger Names

Loggers follow a hierarchical naming convention using dots:

```python
import logging

# Root logger
root_logger = logging.getLogger()

# Application loggers (hierarchical)
app_logger = logging.getLogger('myapp')
db_logger = logging.getLogger('myapp.database') 
api_logger = logging.getLogger('myapp.api')
auth_logger = logging.getLogger('myapp.api.auth')

# Child loggers inherit parent configurations
app_logger.setLevel(logging.INFO)
# db_logger automatically inherits INFO level from 'myapp'
```

This hierarchy enables granular control over different application components.

## 2. Use Appropriate Log Levels Strategically

Understanding when to use each log level is crucial for effective debugging and monitoring:

| Level | Numeric Value | When to Use | Production Visibility |
|-------|---------------|-------------|----------------------|
| **DEBUG** | 10 | Detailed diagnostic info, variable states | Usually disabled |
| **INFO** | 20 | General operational messages | Selectively enabled |
| **WARNING** | 30 | Something unexpected, but app continues | Always enabled |
| **ERROR** | 40 | Serious problems, functionality affected | Always enabled |
| **CRITICAL** | 50 | Very serious errors, app may crash | Always enabled |

### Practical Log Level Usage

```python
import logging
import os

# Configure based on environment
log_level = os.getenv('LOG_LEVEL', 'WARNING').upper()
logging.basicConfig(level=getattr(logging, log_level))
logger = logging.getLogger(__name__)

def authenticate_user(username, password):
    logger.debug(f"Authentication attempt for user: {username}")
    
    user = get_user(username)
    if not user:
        logger.warning(f"Authentication failed - user not found: {username}")
        return None
    
    if not verify_password(user, password):
        logger.error(f"Authentication failed - invalid password for user: {username}")
        return None
    
    if user.is_locked:
        logger.error(f"Authentication blocked - account locked: {username}")
        return None
    
    logger.info(f"User authenticated successfully: {username}")
    return user

def critical_system_check():
    try:
        database_connection = check_database()
        if not database_connection:
            logger.critical("Database connection failed - system cannot operate")
            raise SystemExit("Critical system failure")
    except Exception as e:
        logger.critical(f"Critical system check failed: {e}", exc_info=True)
        raise
```

## 3. Create Module-Specific Named Loggers

Always avoid the root logger in favor of named loggers that provide better control and context:

```python
# âŒ Avoid root logger
import logging
logging.error("Something went wrong")  # No context about source

# âœ… Use named loggers
import logging

# Best practice: use __name__ for automatic module naming
logger = logging.getLogger(__name__)

class DatabaseManager:
    def __init__(self):
        # Create component-specific loggers
        self.logger = logging.getLogger(f"{__name__}.DatabaseManager")
    
    def connect(self):
        self.logger.debug("Attempting database connection")
        try:
            # Connection logic
            self.logger.info("Database connection established")
        except Exception as e:
            self.logger.error("Database connection failed", exc_info=True)
            raise

class APIHandler:
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.APIHandler")
    
    def process_request(self, request):
        self.logger.info(f"Processing {request.method} request to {request.path}")
        # Request processing logic
```

This approach provides clear traceability of log messages to their source components.

## 4. Implement Structured Logging for Better Analysis

Structured logging uses consistent data formats (typically JSON) instead of free-form text, making logs machine-readable and searchable:

```python
import logging
import json
from datetime import datetime

class StructuredFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        
        # Include extra fields if present
        if hasattr(record, 'user_id'):
            log_entry['user_id'] = record.user_id
        if hasattr(record, 'request_id'):
            log_entry['request_id'] = record.request_id
        if hasattr(record, 'duration_ms'):
            log_entry['duration_ms'] = record.duration_ms
            
        return json.dumps(log_entry)

# Configure structured logging
logger = logging.getLogger(__name__)
handler = logging.StreamHandler()
handler.setFormatter(StructuredFormatter())
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# Usage with structured data
def process_order(order_id, user_id):
    start_time = time.time()
    
    logger.info("Order processing started", extra={
        'order_id': order_id,
        'user_id': user_id,
        'event_type': 'order_start'
    })
    
    # Processing logic
    duration_ms = (time.time() - start_time) * 1000
    
    logger.info("Order processing completed", extra={
        'order_id': order_id,
        'user_id': user_id,
        'duration_ms': duration_ms,
        'event_type': 'order_complete'
    })
```

## 5. Secure Your Logs: Avoid Sensitive Information

Accidentally logging sensitive data is a major security risk. Implement automatic redaction:

```python
import logging
import re

class SensitiveDataFilter(logging.Filter):
    """Filter to redact sensitive information from logs"""
    
    SENSITIVE_PATTERNS = {
        'credit_card': re.compile(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'),
        'ssn': re.compile(r'\b\d{3}-?\d{2}-?\d{4}\b'),
        'password': re.compile(r'password["\s]*[:=]["\s]*[^"\s,}]+', re.IGNORECASE),
        'api_key': re.compile(r'api[_-]?key["\s]*[:=]["\s]*[^"\s,}]+', re.IGNORECASE),
        'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
    }
    
    SENSITIVE_KEYS = {
        'password', 'passwd', 'secret', 'token', 'api_key', 
        'private_key', 'credit_card', 'ssn', 'social_security'
    }
    
    def filter(self, record):
        # Redact message content
        record.msg = self._redact_text(str(record.msg))
        
        # Redact extra fields
        if hasattr(record, '__dict__'):
            for key, value in record.__dict__.items():
                if key.lower() in self.SENSITIVE_KEYS:
                    setattr(record, key, '***REDACTED***')
                elif isinstance(value, str):
                    setattr(record, key, self._redact_text(value))
        
        return True
    
    def _redact_text(self, text: str) -> str:
        """Redact sensitive patterns in text"""
        for pattern_name, pattern in self.SENSITIVE_PATTERNS.items():
            text = pattern.sub('***REDACTED***', text)
        return text

# Configure secure logging
logger = logging.getLogger(__name__)
handler = logging.StreamHandler()
handler.addFilter(SensitiveDataFilter())
logger.addHandler(handler)

# Example usage
def process_login(username, password, user_data):
    # This will be automatically redacted
    logger.info(f"Login attempt for {username} with password {password}")
    
    # Structured logging with sensitive data
    logger.info("User data received", extra={
        'username': username,
        'password': password,  # Will be redacted
        'credit_card': user_data.get('credit_card'),  # Will be redacted
        'email': user_data.get('email')  # Will be redacted
    })
```

## 6. Optimize Performance with Lazy Logging

Logging can significantly impact performance if not implemented carefully:

```python
import logging
import time
from functools import wraps

logger = logging.getLogger(__name__)

def expensive_debug_info():
    """Simulate expensive operation for debug info"""
    time.sleep(0.1)  # Expensive computation
    return {"complex_data": "generated after expensive operation"}

# âŒ Performance Problem: Always executes expensive operation
def bad_logging_example():
    debug_info = expensive_debug_info()  # Always executed!
    logger.debug(f"Debug info: {debug_info}")

# âœ… Lazy evaluation: Only executes when needed
def good_logging_example():
    if logger.isEnabledFor(logging.DEBUG):
        debug_info = expensive_debug_info()  # Only when DEBUG enabled
        logger.debug(f"Debug info: {debug_info}")

# Performance monitoring decorator
def log_performance(logger):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                duration = (time.time() - start_time) * 1000
                logger.info(f"{func.__name__} completed in {duration:.2f}ms")
                return result
            except Exception as e:
                duration = (time.time() - start_time) * 1000
                logger.error(f"{func.__name__} failed after {duration:.2f}ms", exc_info=True)
                raise
        return wrapper
    return decorator

# Usage
@log_performance(logger)
def process_data(data):
    # Your processing logic here
    pass
```

## 7. Configure Centralized Logging

Centralized configuration ensures consistency across your application:

### JSON Configuration File (logging_config.json)

```json
{
    "version": 1,
    "disable_existing_loggers": false,
    "formatters": {
        "standard": {
            "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
        },
        "detailed": {
            "format": "%(asctime)s [%(levelname)s] %(name)s:%(lineno)d: %(message)s"
        },
        "json": {
            "()": "myapp.logging.StructuredFormatter"
        }
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "level": "INFO",
            "formatter": "standard",
            "stream": "ext://sys.stdout"
        },
        "file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "DEBUG",
            "formatter": "detailed",
            "filename": "app.log",
            "maxBytes": 10485760,
            "backupCount": 5
        },
        "json_file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "INFO",
            "formatter": "json",
            "filename": "app.json.log",
            "maxBytes": 10485760,
            "backupCount": 5
        }
    },
    "loggers": {
        "myapp": {
            "level": "DEBUG",
            "handlers": ["console", "file", "json_file"],
            "propagate": false
        },
        "myapp.database": {
            "level": "INFO",
            "handlers": ["file"],
            "propagate": false
        },
        "requests": {
            "level": "WARNING",
            "handlers": ["file"],
            "propagate": false
        }
    },
    "root": {
        "level": "WARNING",
        "handlers": ["console"]
    }
}
```

### Configuration Loader

```python
import logging.config
import json
import os
from pathlib import Path

def setup_logging(config_path: str = None, default_level: int = logging.INFO):
    """Setup logging configuration"""
    
    if config_path is None:
        config_path = os.getenv('LOG_CONFIG_PATH', 'logging_config.json')
    
    config_file = Path(config_path)
    
    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
            logging.config.dictConfig(config)
            print(f"Logging configured from {config_path}")
        except Exception as e:
            print(f"Error loading logging config: {e}")
            logging.basicConfig(level=default_level)
    else:
        print(f"Logging config file {config_path} not found, using basic config")
        logging.basicConfig(level=default_level)

# Use in your main application
if __name__ == "__main__":
    setup_logging()
    logger = logging.getLogger(__name__)
    logger.info("Application starting...")
```

## 8. Implement Log Rotation to Manage Disk Space

Log rotation prevents log files from consuming excessive disk space:

```python
import logging
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
import os

# Size-based rotation
def setup_rotating_logger(name: str, log_file: str, max_bytes: int = 10*1024*1024, backup_count: int = 5):
    """Setup logger with size-based rotation"""
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)
    
    # Create logs directory if it doesn't exist
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    handler = RotatingFileHandler(
        log_file, 
        maxBytes=max_bytes,  # 10MB
        backupCount=backup_count
    )
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    return logger

# Time-based rotation
def setup_timed_rotating_logger(name: str, log_file: str):
    """Setup logger with time-based rotation"""
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)
    
    # Create logs directory if it doesn't exist
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    # Rotate daily at midnight, keep 30 days of logs
    handler = TimedRotatingFileHandler(
        log_file,
        when='midnight',
        interval=1,
        backupCount=30
    )
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    return logger

# Example usage
app_logger = setup_rotating_logger('myapp', 'logs/app.log')
access_logger = setup_timed_rotating_logger('myapp.access', 'logs/access.log')
```

## 9. Handle Exceptions Gracefully with Proper Logging

Exception handling and logging work together to provide clear error reporting:

```python
import logging
from functools import wraps
from typing import Optional

logger = logging.getLogger(__name__)

class ApplicationError(Exception):
    """Custom application exception with logging context"""
    def __init__(self, message: str, error_code: str = None, context: dict = None):
        self.message = message
        self.error_code = error_code or "GENERAL_ERROR"
        self.context = context or {}
        super().__init__(message)

def log_exceptions(logger, reraise=True):
    """Decorator to automatically log exceptions"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except ApplicationError as e:
                logger.error(
                    f"Application error in {func.__name__}: {e.message}",
                    extra={
                        'error_code': e.error_code,
                        'context': e.context,
                        'function': func.__name__
                    },
                    exc_info=True
                )
                if reraise:
                    raise
            except Exception as e:
                logger.error(
                    f"Unexpected error in {func.__name__}: {str(e)}",
                    extra={'function': func.__name__},
                    exc_info=True
                )
                if reraise:
                    raise
        return wrapper
    return decorator

@log_exceptions(logger)
def process_user_data(user_data: dict):
    """Example function with comprehensive error handling"""
    
    # Input validation with specific error logging
    if not user_data.get('email'):
        raise ApplicationError(
            "Email is required",
            error_code="VALIDATION_ERROR",
            context={'missing_field': 'email', 'user_data_keys': list(user_data.keys())}
        )
    
    try:
        # Database operation
        result = save_user_to_database(user_data)
        logger.info(
            "User data processed successfully",
            extra={
                'user_email': user_data['email'],
                'user_id': result.get('user_id'),
                'operation': 'create_user'
            }
        )
        return result
        
    except DatabaseConnectionError as e:
        # Specific database error handling
        logger.error(
            "Database connection failed during user creation",
            extra={
                'user_email': user_data['email'],
                'database_error': str(e),
                'retry_recommended': True
            },
            exc_info=True
        )
        raise ApplicationError(
            "Unable to save user data due to database issues",
            error_code="DATABASE_ERROR",
            context={'original_error': str(e), 'user_email': user_data['email']}
        )

def safe_divide(a: float, b: float) -> Optional[float]:
    """Example of exception handling with context"""
    try:
        result = a / b
        logger.debug(f"Division successful: {a} / {b} = {result}")
        return result
    except ZeroDivisionError:
        logger.error(
            "Division by zero attempted",
            extra={
                'dividend': a,
                'divisor': b,
                'operation': 'division'
            }
        )
        return None
    except Exception as e:
        logger.error(
            f"Unexpected error in division: {str(e)}",
            extra={
                'dividend': a,
                'divisor': b,
                'error_type': type(e).__name__
            },
            exc_info=True
        )
        return None
```

## 10. Use Correlation IDs for Request Tracking

Correlation IDs help track requests across application components and services:

```python
import logging
import uuid
from contextvars import ContextVar
from functools import wraps

# Context variable for correlation ID
correlation_id: ContextVar[str] = ContextVar('correlation_id', default=None)

class CorrelationFilter(logging.Filter):
    """Add correlation ID to all log records"""
    
    def filter(self, record):
        record.correlation_id = correlation_id.get() or 'no-correlation-id'
        return True

def with_correlation_id(func):
    """Decorator to ensure function runs with correlation ID"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # Generate new correlation ID if not present
        if correlation_id.get() is None:
            new_id = str(uuid.uuid4())
            correlation_id.set(new_id)
        
        return func(*args, **kwargs)
    return wrapper

# Setup logger with correlation
def setup_correlation_logging():
    logger = logging.getLogger('correlated_app')
    logger.setLevel(logging.DEBUG)
    
    # Create handler with correlation support
    handler = logging.StreamHandler()
    handler.addFilter(CorrelationFilter())
    
    formatter = logging.Formatter(
        '%(asctime)s [%(correlation_id)s] %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    return logger

# Example usage
logger = setup_correlation_logging()

@with_correlation_id
def handle_user_request(user_id: str):
    """Simulate handling a user request"""
    logger.info(f"Processing request for user {user_id}")
    
    # Call other services
    fetch_user_data(user_id)
    update_user_activity(user_id)
    
    logger.info(f"Request completed for user {user_id}")

def fetch_user_data(user_id: str):
    logger.debug(f"Fetching data for user {user_id}")
    # Simulate database call
    logger.info(f"User data retrieved for {user_id}")

def update_user_activity(user_id: str):
    logger.debug(f"Updating activity for user {user_id}")
    # Simulate activity update
    logger.info(f"Activity updated for user {user_id}")
```

## 11. Environment-Specific Configuration

Configure logging differently across development, staging, and production:

```python
import logging
import os
from enum import Enum
from typing import Dict, Any

class Environment(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"
    TESTING = "testing"

class EnvironmentConfig:
    """Environment-specific logging configuration"""
    
    @staticmethod
    def get_config() -> Dict[str, Any]:
        env = Environment(os.getenv('APP_ENV', 'development').lower())
        
        base_config = {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "simple": {
                    "format": "%(levelname)s - %(message)s"
                },
                "detailed": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s"
                },
                "json": {
                    "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                    "class": "pythonjsonlogger.jsonlogger.JsonFormatter"
                }
            },
            "handlers": {},
            "loggers": {},
            "root": {
                "level": "WARNING",
                "handlers": []
            }
        }
        
        if env == Environment.DEVELOPMENT:
            return EnvironmentConfig._development_config(base_config)
        elif env == Environment.STAGING:
            return EnvironmentConfig._staging_config(base_config)
        elif env == Environment.PRODUCTION:
            return EnvironmentConfig._production_config(base_config)
        elif env == Environment.TESTING:
            return EnvironmentConfig._testing_config(base_config)
        
        return base_config
    
    @staticmethod
    def _development_config(base_config):
        """Development environment configuration"""
        base_config.update({
            "handlers": {
                "console": {
                    "class": "logging.StreamHandler",
                    "formatter": "detailed",
                    "level": "DEBUG",
                    "stream": "ext://sys.stdout"
                },
                "file": {
                    "class": "logging.handlers.RotatingFileHandler",
                    "formatter": "detailed",
                    "filename": "logs/dev.log",
                    "maxBytes": 10485760,
                    "backupCount": 3,
                    "level": "DEBUG"
                }
            },
            "loggers": {
                "myapp": {
                    "level": "DEBUG",
                    "handlers": ["console", "file"],
                    "propagate": False
                }
            },
            "root": {
                "level": "DEBUG",
                "handlers": ["console"]
            }
        })
        return base_config
    
    @staticmethod
    def _production_config(base_config):
        """Production environment configuration"""
        base_config.update({
            "handlers": {
                "console": {
                    "class": "logging.StreamHandler",
                    "formatter": "json",
                    "level": "WARNING"
                },
                "file": {
                    "class": "logging.handlers.RotatingFileHandler",
                    "formatter": "json",
                    "filename": "logs/production.log",
                    "maxBytes": 104857600,  # 100MB
                    "backupCount": 20,
                    "level": "INFO"
                }
            },
            "loggers": {
                "myapp": {
                    "level": "INFO",
                    "handlers": ["console", "file"],
                    "propagate": False
                }
            },
            "root": {
                "level": "WARNING",
                "handlers": ["console"]
            }
        })
        return base_config

# Usage
def setup_environment_logging():
    """Setup logging based on current environment"""
    import logging.config
    
    config = EnvironmentConfig.get_config()
    logging.config.dictConfig(config)
    
    logger = logging.getLogger(__name__)
    logger.info(f"Logging configured for environment: {os.getenv('APP_ENV', 'development')}")
    
    return logger
```

## 12. Monitor and Alert on Log Patterns

Set up monitoring and alerting based on log patterns:

```python
import logging
import re
from collections import defaultdict
from datetime import datetime, timedelta
from typing import Dict
import threading
import time

class LogMonitor:
    """Monitor logs for specific patterns and trigger alerts"""
    
    def __init__(self, alert_callback=None):
        self.alert_callback = alert_callback or self._default_alert
        self.error_counts = defaultdict(int)
        self.thresholds = {
            'error_rate': 10,  # errors per minute
            'critical_errors': 1,  # immediate alert
            'repeated_errors': 5   # same error repeated
        }
        self.error_patterns = {
            'database_error': re.compile(r'database.*error|connection.*failed', re.IGNORECASE),
            'authentication_error': re.compile(r'auth.*failed|unauthorized', re.IGNORECASE),
            'timeout_error': re.compile(r'timeout|timed out', re.IGNORECASE),
        }
    
    def _default_alert(self, alert_type: str, message: str):
        """Default alert handler"""
        print(f"ALERT [{alert_type}]: {message}")
    
    def check_log_message(self, record):
        """Check log message for alert conditions"""
        if record.levelno >= logging.ERROR:
            message = record.getMessage()
            
            # Check for critical errors
            if record.levelno >= logging.CRITICAL:
                self._trigger_alert('CRITICAL_ERROR', message)
            
            # Check for specific error patterns
            for pattern_name, pattern in self.error_patterns.items():
                if pattern.search(message):
                    self.error_counts[pattern_name] += 1
                    
                    if self.error_counts[pattern_name] >= self.thresholds['repeated_errors']:
                        self._trigger_alert(
                            'REPEATED_ERROR',
                            f"Repeated {pattern_name}: {message}"
                        )
                        # Reset counter after alert
                        self.error_counts[pattern_name] = 0
    
    def _trigger_alert(self, alert_type: str, message: str):
        """Trigger an alert"""
        self.alert_callback(alert_type, message)

class AlertingHandler(logging.Handler):
    """Custom logging handler that triggers alerts"""
    
    def __init__(self, monitor: LogMonitor):
        super().__init__()
        self.monitor = monitor
    
    def emit(self, record):
        """Handle log record and check for alert conditions"""
        self.monitor.check_log_message(record)

# Setup monitoring
def setup_monitored_logging():
    """Setup logging with monitoring and alerting"""
    
    def custom_alert_handler(alert_type: str, message: str):
        print(f"ðŸš¨ ALERT [{alert_type}]: {message}")
    
    monitor = LogMonitor(alert_callback=custom_alert_handler)
    
    # Create logger with alerting handler
    logger = logging.getLogger('monitored_app')
    logger.setLevel(logging.DEBUG)
    
    # Add standard handler
    console_handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # Add alerting handler
    alerting_handler = AlertingHandler(monitor)
    logger.addHandler(alerting_handler)
    
    return logger, monitor

# Example usage
logger, monitor = setup_monitored_logging()

def simulate_errors():
    """Simulate various types of errors for testing"""
    logger.error("Database connection failed: timeout after 30 seconds")
    logger.error("Authentication failed for user john.doe@example.com")
    logger.critical("Out of memory: cannot allocate buffer")
    
    # Simulate repeated errors
    for i in range(6):
        logger.error("Database connection failed: server unavailable")
```

## 13. Test Your Logging Implementation

Testing ensures your logging works correctly across different scenarios:

```python
import logging
import io
import unittest
from unittest.mock import patch
import json

class TestLogging(unittest.TestCase):
    
    def setUp(self):
        # Create a logger for testing
        self.logger = logging.getLogger('test_logger')
        self.logger.setLevel(logging.DEBUG)
        
        # Create a string stream to capture log output
        self.log_stream = io.StringIO()
        self.handler = logging.StreamHandler(self.log_stream)
        self.logger.addHandler(self.handler)
    
    def tearDown(self):
        # Clean up handlers
        self.logger.removeHandler(self.handler)
        self.handler.close()
    
    def test_log_levels(self):
        """Test that different log levels work correctly"""
        self.logger.debug("Debug message")
        self.logger.info("Info message")
        self.logger.warning("Warning message")
        self.logger.error("Error message")
        self.logger.critical("Critical message")
        
        log_output = self.log_stream.getvalue()
        self.assertIn("Debug message", log_output)
        self.assertIn("Info message", log_output)
        self.assertIn("WARNING", log_output)
        self.assertIn("ERROR", log_output)
        self.assertIn("CRITICAL", log_output)
    
    def test_structured_logging(self):
        """Test structured logging with extra fields"""
        formatter = logging.Formatter('%(message)s - %(user_id)s')
        self.handler.setFormatter(formatter)
        
        self.logger.info("User action", extra={'user_id': '12345'})
        
        log_output = self.log_stream.getvalue()
        self.assertIn("User action - 12345", log_output)
    
    def test_exception_logging(self):
        """Test exception logging with stack traces"""
        try:
            raise ValueError("Test exception")
        except ValueError:
            self.logger.exception("An error occurred")
        
        log_output = self.log_stream.getvalue()
        self.assertIn("An error occurred", log_output)
        self.assertIn("ValueError: Test exception", log_output)
        self.assertIn("Traceback", log_output)
    
    def test_performance_impact(self):
        """Test logging performance impact"""
        import time
        
        logger = logging.getLogger('perf_test')
        logger.setLevel(logging.INFO)
        
        # Test with logging disabled
        start_time = time.time()
        for i in range(1000):
            logger.debug(f"Debug message {i}")  # Should be ignored
        disabled_time = time.time() - start_time
        
        # Test with logging enabled
        start_time = time.time()
        for i in range(1000):
            logger.info(f"Info message {i}")  # Should be processed
        enabled_time = time.time() - start_time
        
        # Disabled logging should be much faster
        self.assertLess(disabled_time, enabled_time * 0.1)

if __name__ == '__main__':
    unittest.main()
```

## 14. Integrate with SigNoz for Comprehensive Observability

SigNoz provides powerful log management with correlation to traces and metrics. Here's how to integrate Python logging with SigNoz using OpenTelemetry:

```python
import logging
import json
from datetime import datetime
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.exporter.otlp.proto.grpc._log_exporter import OTLPLogExporter
from opentelemetry.instrumentation.logging import LoggingInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk._logs import LoggerProvider, LoggingHandler
from opentelemetry.sdk._logs.export import BatchLogRecordProcessor

class SigNozLogFormatter(logging.Formatter):
    """Custom formatter for SigNoz integration"""
    
    def format(self, record):
        # Create structured log entry
        log_entry = {
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'level': record.levelname,
            'logger_name': record.name,
            'message': record.getMessage(),
            'module': getattr(record, 'module', ''),
            'function': getattr(record, 'funcName', ''),
            'line_number': getattr(record, 'lineno', 0)
        }
        
        # Add custom attributes
        for key, value in record.__dict__.items():
            if key.startswith('custom_') or key in ['user_id', 'request_id', 'trace_id', 'span_id']:
                log_entry[key] = value
        
        # Add exception information if present
        if record.exc_info:
            log_entry['exception'] = {
                'type': record.exc_info[0].__name__ if record.exc_info[0] else None,
                'message': str(record.exc_info[1]) if record.exc_info[1] else None,
                'traceback': self.formatException(record.exc_info)
            }
        
        return json.dumps(log_entry)

def setup_signoz_logging(
    signoz_endpoint: str = "http://localhost:4317",
    service_name: str = "python-app",
    service_version: str = "1.0.0"
):
    """Setup logging integration with SigNoz"""
    
    # Create resource with service information
    resource = Resource.create({
        "service.name": service_name,
        "service.version": service_version,
        "deployment.environment": "production"
    })
    
    # Setup OpenTelemetry tracing
    trace.set_tracer_provider(TracerProvider(resource=resource))
    tracer = trace.get_tracer(__name__)
    
    # Setup trace exporter
    span_exporter = OTLPSpanExporter(endpoint=signoz_endpoint, insecure=True)
    span_processor = BatchSpanProcessor(span_exporter)
    trace.get_tracer_provider().add_span_processor(span_processor)
    
    # Setup logging
    logger_provider = LoggerProvider(resource=resource)
    
    # Create OTLP log exporter
    log_exporter = OTLPLogExporter(endpoint=signoz_endpoint, insecure=True)
    log_processor = BatchLogRecordProcessor(log_exporter)
    logger_provider.add_log_record_processor(log_processor)
    
    # Create OpenTelemetry logging handler
    otel_handler = LoggingHandler(logger_provider=logger_provider)
    otel_handler.setFormatter(SigNozLogFormatter())
    
    # Instrument the logging module
    LoggingInstrumentor().instrument(set_logging_format=True)
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.addHandler(otel_handler)
    root_logger.setLevel(logging.INFO)
    
    return tracer, logger_provider

# Example application with SigNoz integration
class ECommerceApp:
    """Example e-commerce application with comprehensive logging"""
    
    def __init__(self):
        self.tracer, self.logger_provider = setup_signoz_logging(
            service_name="ecommerce-app", 
            service_version="2.1.0"
        )
        self.logger = logging.getLogger(__name__)
    
    def process_order(self, user_id: str, order_data: dict):
        """Process an order with distributed tracing and structured logging"""
        
        with self.tracer.start_as_current_span("process_order") as span:
            # Add span attributes
            span.set_attribute("user.id", user_id)
            span.set_attribute("order.id", order_data['id'])
            span.set_attribute("order.amount", order_data['amount'])
            
            # Log with correlation
            self.logger.info("Order processing started", extra={
                'user_id': user_id,
                'order_id': order_data['id'],
                'order_amount': order_data['amount'],
                'trace_id': format(span.get_span_context().trace_id, '032x'),
                'span_id': format(span.get_span_context().span_id, '016x')
            })
            
            try:
                # Validate order
                self._validate_order(order_data, span)
                
                # Process payment
                payment_result = self._process_payment(user_id, order_data, span)
                
                self.logger.info("Order processing completed successfully", extra={
                    'user_id': user_id,
                    'order_id': order_data['id'],
                    'payment_id': payment_result.get('payment_id'),
                    'trace_id': format(span.get_span_context().trace_id, '032x')
                })
                
                span.set_attribute("order.status", "success")
                return {"status": "success", "order_id": order_data['id']}
                
            except Exception as e:
                self.logger.error("Order processing failed", extra={
                    'user_id': user_id,
                    'order_id': order_data['id'],
                    'error_type': type(e).__name__,
                    'error_message': str(e),
                    'trace_id': format(span.get_span_context().trace_id, '032x')
                }, exc_info=True)
                
                span.record_exception(e)
                span.set_attribute("order.status", "failed")
                raise
    
    def _validate_order(self, order_data: dict, parent_span):
        """Validate order data"""
        with self.tracer.start_as_current_span("validate_order", parent=parent_span) as span:
            self.logger.debug("Validating order data", extra={'order_id': order_data['id']})
            
            if order_data['amount'] <= 0:
                error_msg = "Invalid order amount"
                self.logger.error(error_msg, extra={
                    'order_id': order_data['id'],
                    'invalid_amount': order_data['amount']
                })
                raise ValueError(error_msg)
            
            span.set_attribute("validation.status", "passed")
            self.logger.debug("Order validation passed", extra={'order_id': order_data['id']})
    
    def _process_payment(self, user_id: str, order_data: dict, parent_span):
        """Process payment with detailed logging"""
        with self.tracer.start_as_current_span("process_payment", parent=parent_span) as span:
            payment_id = f"pay_{int(time.time())}"
            
            span.set_attribute("payment.id", payment_id)
            span.set_attribute("payment.amount", order_data['amount'])
            
            self.logger.info("Processing payment", extra={
                'user_id': user_id,
                'order_id': order_data['id'],
                'payment_id': payment_id,
                'amount': order_data['amount']
            })
            
            if order_data['amount'] > 10000:
                self.logger.warning("High-value payment processed", extra={
                    'user_id': user_id,
                    'payment_id': payment_id,
                    'amount': order_data['amount'],
                    'requires_review': True
                })
            
            return {'payment_id': payment_id, 'status': 'completed'}

# Example usage
def main():
    """Main application with SigNoz logging"""
    app = ECommerceApp()
    
    # Sample order for testing
    sample_order = {
        "id": "order_001",
        "amount": 99.99,
        "payment_method": "credit_card"
    }
    
    try:
        result = app.process_order("user_001", sample_order)
        print(f"Order processed: {result}")
    except Exception as e:
        print(f"Order failed: {e}")

if __name__ == "__main__":
    main()
```

## Get Started with SigNoz

You can choose between various deployment options in SigNoz. The easiest way to get started with SigNoz is [SigNoz cloud](https://signoz.io/teams/). We offer a 30-day free trial account with access to all features.

Those who have data privacy concerns and can't send their data outside their infrastructure can sign up for either [enterprise self-hosted or BYOC offering](https://signoz.io/contact-us/).

Those who have the expertise to manage SigNoz themselves or just want to start with a free self-hosted option can use our [community edition](https://signoz.io/docs/install/self-host/).

### SigNoz Features for Python Logging

SigNoz provides several features that make it ideal for Python logging:

- **Unified Observability**: Correlate logs with traces and metrics in a single platform
- **Advanced Search**: Search through logs using complex queries and filters  
- **Real-time Monitoring**: Stream logs in real-time with customizable dashboards
- **Alerting**: Set up alerts based on log patterns, error rates, or custom queries
- **Performance Analysis**: Analyze application performance through correlated logs and traces
- **Cost Effective**: Open-source alternative to expensive commercial observability platforms

Hope we answered all your questions regarding Python logging best practices. If you have more questions, feel free to use the SigNoz AI chatbot, or join our [slack community](https://signoz.io/slack/).

## 15. Advanced Error Recovery Patterns

Implement sophisticated error handling that provides both logging and recovery mechanisms:

```python
import logging
import functools
import time
from typing import Type, Tuple, Callable
from enum import Enum
from dataclasses import dataclass

class ErrorSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high" 
    CRITICAL = "critical"

@dataclass
class ErrorContext:
    """Context information for error handling"""
    operation: str
    user_id: str = None
    request_id: str = None
    retry_count: int = 0
    max_retries: int = 3
    severity: ErrorSeverity = ErrorSeverity.MEDIUM

class CircuitBreaker:
    """Circuit breaker pattern with logging"""
    
    def __init__(self, failure_threshold: int = 5, reset_timeout: int = 60, logger: logging.Logger = None):
        self.failure_threshold = failure_threshold
        self.reset_timeout = reset_timeout
        self.logger = logger or logging.getLogger(__name__)
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
    
    def __call__(self, func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            if self.state == "OPEN":
                if time.time() - self.last_failure_time > self.reset_timeout:
                    self.state = "HALF_OPEN"
                    self.logger.info("Circuit breaker half-open", extra={
                        'function': func.__name__,
                        'failure_count': self.failure_count
                    })
                else:
                    self.logger.warning("Circuit breaker open - request rejected", extra={
                        'function': func.__name__,
                        'failure_count': self.failure_count
                    })
                    raise Exception(f"Circuit breaker OPEN for {func.__name__}")
            
            try:
                result = func(*args, **kwargs)
                if self.state == "HALF_OPEN":
                    self.state = "CLOSED"
                    self.failure_count = 0
                    self.logger.info("Circuit breaker closed - service recovered", extra={
                        'function': func.__name__
                    })
                return result
                
            except Exception as e:
                self.failure_count += 1
                self.last_failure_time = time.time()
                
                if self.failure_count >= self.failure_threshold:
                    self.state = "OPEN"
                    self.logger.error("Circuit breaker opened", extra={
                        'function': func.__name__,
                        'failure_count': self.failure_count,
                        'error': str(e)
                    }, exc_info=True)
                
                raise
                
        return wrapper

def retry_with_backoff(
    max_retries: int = 3,
    backoff_factor: float = 1.0,
    exceptions: Tuple[Type[Exception], ...] = (Exception,),
    logger: logging.Logger = None
):
    """Retry decorator with exponential backoff and comprehensive logging"""
    
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            retry_logger = logger or logging.getLogger(func.__module__)
            
            for attempt in range(max_retries + 1):
                try:
                    if attempt > 0:
                        wait_time = backoff_factor * (2 ** (attempt - 1))
                        retry_logger.info("Retrying operation", extra={
                            'function': func.__name__,
                            'attempt': attempt + 1,
                            'max_retries': max_retries,
                            'wait_time': wait_time
                        })
                        time.sleep(wait_time)
                    
                    result = func(*args, **kwargs)
                    
                    if attempt > 0:
                        retry_logger.info("Operation succeeded after retry", extra={
                            'function': func.__name__,
                            'successful_attempt': attempt + 1
                        })
                    
                    return result
                    
                except exceptions as e:
                    last_exception = e
                    
                    if attempt < max_retries:
                        retry_logger.warning("Operation failed, will retry", extra={
                            'function': func.__name__,
                            'attempt': attempt + 1,
                            'error_type': type(e).__name__,
                            'error_message': str(e)
                        })
                    else:
                        retry_logger.error("Operation failed after all retries", extra={
                            'function': func.__name__,
                            'total_attempts': attempt + 1,
                            'final_error': str(e)
                        }, exc_info=True)
                        raise
                        
        return wrapper
    return decorator

# Example usage with comprehensive error handling
class PaymentService:
    """Payment service with advanced error handling"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.circuit_breaker = CircuitBreaker(failure_threshold=3, logger=self.logger)
    
    @circuit_breaker  
    @retry_with_backoff(max_retries=2, exceptions=(ConnectionError, TimeoutError))
    def process_payment(self, user_id: str, amount: float) -> dict:
        """Process payment with comprehensive error handling"""
        
        self.logger.info("Processing payment", extra={
            'user_id': user_id,
            'amount': amount
        })
        
        try:
            # Simulate payment processing
            if amount <= 0:
                raise ValueError("Invalid payment amount")
            
            # Simulate processing
            result = {
                'payment_id': f"pay_{int(time.time())}",
                'status': 'completed',
                'amount': amount,
                'user_id': user_id
            }
            
            self.logger.info("Payment processed successfully", extra={
                'payment_id': result['payment_id'],
                'user_id': user_id
            })
            
            return result
            
        except ValueError as e:
            self.logger.error("Payment validation failed", extra={
                'user_id': user_id,
                'amount': amount,
                'error': str(e)
            })
            raise
        except Exception as e:
            self.logger.error("Unexpected payment error", extra={
                'user_id': user_id,
                'amount': amount,
                'error': str(e)
            }, exc_info=True)
            raise

# Example usage
payment_service = PaymentService()

try:
    result = payment_service.process_payment("user123", 100.0)
    print(f"Payment result: {result}")
except Exception as e:
    print(f"Payment failed: {e}")
```

## 16. Implement Log Sampling for High-Volume Applications

For high-throughput applications, implement log sampling to reduce volume while maintaining visibility:

```python
import random
import logging
from collections import defaultdict
from typing import Dict

class SamplingHandler(logging.Handler):
    """Handler that samples log messages based on configurable rules"""
    
    def __init__(self, base_handler, sample_rates: Dict[int, float] = None):
        super().__init__()
        self.base_handler = base_handler
        # Default sample rates by log level
        self.sample_rates = sample_rates or {
            logging.DEBUG: 0.01,    # 1% of debug messages
            logging.INFO: 0.1,      # 10% of info messages  
            logging.WARNING: 0.5,   # 50% of warning messages
            logging.ERROR: 1.0,     # 100% of error messages
            logging.CRITICAL: 1.0   # 100% of critical messages
        }
        
        # Track sampling statistics
        self.stats = defaultdict(lambda: {'total': 0, 'sampled': 0})
    
    def emit(self, record):
        """Emit record based on sampling rate"""
        level = record.levelno
        sample_rate = self.sample_rates.get(level, 1.0)
        
        self.stats[level]['total'] += 1
        
        if random.random() <= sample_rate:
            self.stats[level]['sampled'] += 1
            # Add sampling metadata to record
            record.sampled = True
            record.sample_rate = sample_rate
            self.base_handler.emit(record)
    
    def get_sampling_stats(self):
        """Get sampling statistics"""
        return dict(self.stats)

# Usage example
def setup_sampling_logger():
    """Setup logger with sampling"""
    
    # Create base handler
    base_handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    base_handler.setFormatter(formatter)
    
    # Create sampling handler
    sampling_handler = SamplingHandler(
        base_handler,
        sample_rates={
            logging.DEBUG: 0.01,   # 1% sampling
            logging.INFO: 0.1,     # 10% sampling
            logging.WARNING: 0.5,  # 50% sampling
            logging.ERROR: 1.0,    # 100% sampling
            logging.CRITICAL: 1.0  # 100% sampling
        }
    )
    
    # Setup logger
    logger = logging.getLogger('sampled_app')
    logger.setLevel(logging.DEBUG)
    logger.addHandler(sampling_handler)
    
    return logger, sampling_handler

# High-volume logging example
logger, sampling_handler = setup_sampling_logger()

def process_high_volume_events():
    """Simulate processing high volume of events with sampling"""
    
    for i in range(10000):  # Simulate 10k events
        if i % 1000 == 0:
            logger.error(f"Error processing batch {i}")
        elif i % 100 == 0:
            logger.warning(f"Warning at event {i}")
        elif i % 10 == 0:
            logger.info(f"Processing batch {i}")
        else:
            logger.debug(f"Processing event {i}")
    
    # Print sampling statistics
    print("Sampling Statistics:")
    stats = sampling_handler.get_sampling_stats()
    for level, data in stats.items():
        level_name = logging.getLevelName(level)
        sample_rate = (data['sampled'] / data['total']) * 100 if data['total'] > 0 else 0
        print(f"{level_name}: {data['sampled']}/{data['total']} ({sample_rate:.1f}%)")

if __name__ == "__main__":
    process_high_volume_events()
```

## 17. Dynamic Log Level Management

Implement dynamic log level changes without application restart:

```python
import logging
import json
import threading
from pathlib import Path
from datetime import datetime

class DynamicLogLevelManager:
    """Manage log levels dynamically through configuration files"""
    
    def __init__(self, config_file: str = "log_levels.json"):
        self.config_file = Path(config_file)
        self.loggers = {}
        self.lock = threading.RLock()
        
        # Create default config if it doesn't exist
        self._create_default_config()
    
    def register_logger(self, logger: logging.Logger, default_level: int = logging.INFO):
        """Register a logger for dynamic management"""
        with self.lock:
            self.loggers[logger.name] = logger
            self._apply_config()
    
    def set_log_level(self, logger_name: str, level: int):
        """Set log level for a specific logger"""
        with self.lock:
            if logger_name in self.loggers:
                logger = self.loggers[logger_name]
                old_level = logger.level
                logger.setLevel(level)
                
                logger.info(f"Log level changed from {logging.getLevelName(old_level)} to {logging.getLevelName(level)}")
    
    def _create_default_config(self):
        """Create default configuration file"""
        if not self.config_file.exists():
            default_config = {
                "log_levels": {
                    "root": "WARNING",
                    "myapp": "INFO",
                    "myapp.database": "WARNING"
                },
                "last_updated": datetime.now().isoformat()
            }
            
            self.config_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_file, 'w') as f:
                json.dump(default_config, f, indent=2)
    
    def _apply_config(self):
        """Apply configuration from file"""
        try:
            with open(self.config_file, 'r') as f:
                config = json.load(f)
            
            log_levels = config.get('log_levels', {})
            
            for logger_name, level_name in log_levels.items():
                if logger_name in self.loggers:
                    level = getattr(logging, level_name.upper(), logging.INFO)
                    logger = self.loggers[logger_name]
                    
                    if logger.level != level:
                        old_level_name = logging.getLevelName(logger.level)
                        logger.setLevel(level)
                        logger.info(f"Log level updated from {old_level_name} to {level_name}")
        
        except Exception as e:
            print(f"Error applying log configuration: {e}")
    
    def update_config(self, logger_name: str, level_name: str):
        """Update the configuration file with new log level"""
        with self.lock:
            try:
                config = {"log_levels": {}}
                if self.config_file.exists():
                    with open(self.config_file, 'r') as f:
                        config = json.load(f)
                
                config['log_levels'][logger_name] = level_name.upper()
                config['last_updated'] = datetime.now().isoformat()
                
                with open(self.config_file, 'w') as f:
                    json.dump(config, f, indent=2)
                    
                # Apply immediately
                self._apply_config()
                    
            except Exception as e:
                print(f"Error updating config: {e}")

# Example usage
def setup_dynamic_logging():
    """Setup application with dynamic log level management"""
    
    # Create log manager
    log_manager = DynamicLogLevelManager("config/log_levels.json")
    
    # Setup application loggers
    app_logger = logging.getLogger('myapp')
    db_logger = logging.getLogger('myapp.database')
    
    # Configure basic logging
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Register loggers with dynamic manager
    log_manager.register_logger(app_logger, logging.INFO)
    log_manager.register_logger(db_logger, logging.WARNING)
    
    return log_manager, {'app': app_logger, 'db': db_logger}

if __name__ == "__main__":
    log_manager, loggers = setup_dynamic_logging()
    
    # Test logging
    loggers['app'].debug("Debug message")
    loggers['app'].info("Info message")
    loggers['db'].debug("DB debug message")
    
    # Change log level dynamically
    log_manager.update_config('myapp', 'DEBUG')
    
    # Test again
    loggers['app'].debug("Debug message after level change")
```

## 18. Context Managers for Scoped Logging

Use context managers to maintain logging context across operations:

```python
import logging
from contextlib import contextmanager
from contextvars import ContextVar
import time
import uuid

# Context variables for maintaining state
request_context: ContextVar[dict] = ContextVar('request_context', default={})

class LogContext:
    """Context manager for scoped logging with automatic cleanup"""
    
    def __init__(self, logger, **context):
        self.logger = logger
        self.context = context
        self.start_time = None
        self.original_context = {}
    
    def __enter__(self):
        self.start_time = time.time()
        
        # Store original context
        current_context = request_context.get({})
        self.original_context = current_context.copy()
        
        # Update context
        new_context = current_context.copy()
        new_context.update(self.context)
        request_context.set(new_context)
        
        # Log entry
        self.logger.info("Operation started", extra=new_context)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time
        context = request_context.get({})
        context['duration_ms'] = round(duration * 1000, 2)
        
        if exc_type is None:
            self.logger.info("Operation completed successfully", extra=context)
        else:
            context['error_type'] = exc_type.__name__
            context['error_message'] = str(exc_val)
            self.logger.error("Operation failed", extra=context, exc_info=True)
        
        # Restore original context
        request_context.set(self.original_context)

@contextmanager
def user_operation(logger, user_id: str, operation: str):
    """Context manager for user operations"""
    operation_id = str(uuid.uuid4())
    
    with LogContext(logger, 
                   user_id=user_id, 
                   operation=operation,
                   operation_id=operation_id):
        yield operation_id

# Example usage
logger = logging.getLogger(__name__)

def process_user_order(user_id: str, order_data: dict):
    """Process user order with comprehensive logging context"""
    
    with user_operation(logger, user_id, "process_order") as operation_id:
        logger.info("Validating order data", extra={
            'order_id': order_data['id'],
            'amount': order_data['amount']
        })
        
        # Validate order
        if order_data['amount'] <= 0:
            logger.error("Invalid order amount", extra={
                'order_id': order_data['id'],
                'amount': order_data['amount']
            })
            raise ValueError("Order amount must be positive")
        
        # Process payment
        logger.info("Processing payment", extra={
            'order_id': order_data['id'],
            'amount': order_data['amount']
        })
        
        # Simulate processing time
        time.sleep(0.1)
        
        logger.info("Order processing completed", extra={
            'order_id': order_data['id']
        })
        
        return {"status": "success", "operation_id": operation_id}

# Example usage
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

order = {"id": "order_123", "amount": 99.99}
try:
    result = process_user_order("user_001", order)
    print(f"Result: {result}")
except Exception as e:
    print(f"Failed: {e}")
```

## 19. Async Logging for High-Performance Applications

For applications with high logging volume, asynchronous logging prevents I/O blocking:

```python
import asyncio
import logging
from logging.handlers import QueueHandler, QueueListener
import queue
import threading

class AsyncLogHandler:
    """Asynchronous logging handler using queue and background thread"""
    
    def __init__(self, handlers=None):
        self.log_queue = queue.Queue(-1)  # Unlimited queue size
        self.handlers = handlers or [logging.StreamHandler()]
        
        # Create queue handler for main thread
        self.queue_handler = QueueHandler(self.log_queue)
        
        # Create queue listener for background processing
        self.queue_listener = QueueListener(self.log_queue, *self.handlers)
        
        # Start the listener thread
        self.queue_listener.start()
    
    def get_handler(self):
        return self.queue_handler
    
    def stop(self):
        self.queue_listener.stop()

# Setup async logging
def setup_async_logging():
    # Create file and console handlers
    file_handler = logging.FileHandler('async_app.log')
    console_handler = logging.StreamHandler()
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # Create async handler
    async_handler = AsyncLogHandler([file_handler, console_handler])
    
    # Configure logger
    logger = logging.getLogger('async_app')
    logger.setLevel(logging.DEBUG)
    logger.addHandler(async_handler.get_handler())
    
    return logger, async_handler

# Example usage with asyncio
async def async_worker(worker_id: int, logger: logging.Logger):
    """Simulate async work with logging"""
    for i in range(100):
        logger.info(f"Worker {worker_id} processing item {i}")
        await asyncio.sleep(0.01)  # Simulate async work
    
    logger.info(f"Worker {worker_id} completed")

async def main():
    logger, async_handler = setup_async_logging()
    
    try:
        # Create multiple async workers
        tasks = [
            async_worker(worker_id, logger) 
            for worker_id in range(5)
        ]
        
        # Run all workers concurrently
        await asyncio.gather(*tasks)
        
        logger.info("All workers completed")
        
    finally:
        # Cleanup async handler
        async_handler.stop()

if __name__ == "__main__":
    asyncio.run(main())
```

## 20. Third-Party Logging Libraries

While Python's built-in logging is powerful, sometimes third-party libraries offer better ergonomics:

### Loguru - Simplified Logging

```python
from loguru import logger
import sys

# Remove default handler
logger.remove()

# Add custom handler with rotation
logger.add(
    "logs/app_{time}.log",
    rotation="100 MB",
    retention="10 days",
    compression="zip",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}"
)

# Add console handler for development
logger.add(
    sys.stdout,
    level="DEBUG",
    format="<green>{time:HH:mm:ss}</green> | <level>{level}</level> | <cyan>{name}</cyan> | {message}"
)

# Structured logging with Loguru
def process_order(order_id, user_id, amount):
    with logger.contextualize(order_id=order_id, user_id=user_id):
        logger.info("Processing order", amount=amount)
        
        try:
            # Order processing logic
            result = validate_and_charge(amount)
            logger.success("Order processed successfully", result=result)
            return result
        except PaymentError as e:
            logger.error("Payment failed", error=str(e), amount=amount)
            raise
        except Exception as e:
            logger.exception("Unexpected error during order processing")
            raise

# Custom sink for external services
def send_to_monitoring(message):
    """Custom sink to send logs to external monitoring"""
    # Parse and send to monitoring service
    pass

logger.add(send_to_monitoring, level="ERROR")
```

### Structlog - Structured Logging

```python
import structlog
import logging

# Configure structlog
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

# Create structured logger
logger = structlog.get_logger(__name__)

def process_user_registration(user_data):
    """Example function using structured logging"""
    
    # Bind context that will be included in all subsequent log messages
    log = logger.bind(user_email=user_data['email'], operation="user_registration")
    
    log.info("Starting user registration")
    
    try:
        # Validate user data
        validation_errors = validate_user_data(user_data)
        if validation_errors:
            log.warning("User data validation failed", errors=validation_errors)
            return {"success": False, "errors": validation_errors}
        
        # Create user
        user_id = create_user_account(user_data)
        log.info("User account created", user_id=user_id)
        
        # Send welcome email
        send_welcome_email(user_data['email'])
        log.info("Welcome email sent")
        
        return {"success": True, "user_id": user_id}
        
    except Exception as e:
        log.exception("Unexpected error during registration")
        raise
```

## Conclusion

Mastering Python logging requires understanding both fundamental concepts and advanced patterns. These 20 best practices will help you:

- **Build maintainable systems** with clear, structured logs
- **Debug production issues** faster with proper correlation and context
- **Scale your applications** using performance-optimized logging patterns
- **Secure your logs** by preventing sensitive data leakage
- **Monitor your systems** proactively with centralized log management

### Key Takeaways:

1. **Always use named loggers** instead of the root logger for better control
2. **Implement structured logging** early for easier analysis and monitoring
3. **Optimize performance** with lazy evaluation and asynchronous logging
4. **Centralize configuration** to maintain consistency across environments
5. **Secure your logs** by automatically redacting sensitive information
6. **Use correlation IDs** to trace requests across distributed systems
7. **Test your logging** implementation to ensure it works under all conditions
8. **Monitor log patterns** and set up alerting for critical issues
9. **Integrate with observability platforms** like SigNoz for comprehensive monitoring
10. **Implement proper error handling** with recovery strategies and circuit breakers

Remember: effective logging is not about capturing every detail, but about capturing the *right* details at the *right* time with the *right* structure. Start with these fundamentals and gradually adopt advanced patterns as your applications scale.

## FAQs

**What is the best way to log in Python?**

The best way to log in Python is using the built-in logging module with named loggers, structured formatting, and appropriate log levels. Always use `logging.getLogger(__name__)` instead of the root logger, and configure centralized logging with proper handlers for different environments.

**How can I improve Python logging performance?**

Improve Python logging performance by: 1) Using lazy evaluation with `logger.isEnabledFor()` checks 2) Implementing asynchronous logging with QueueHandler 3) Avoiding expensive operations in log messages 4) Using appropriate log levels 5) Implementing log sampling for high-volume applications.

**What are the different log levels in Python?**

Python has five standard logging levels: DEBUG (10), INFO (20), WARNING (30), ERROR (40), and CRITICAL (50). Use DEBUG for detailed diagnostic information, INFO for general messages, WARNING for unexpected situations, ERROR for serious problems, and CRITICAL for very serious errors.

**How do I implement structured logging in Python?**

Implement structured logging by creating custom formatters that output JSON, using the `extra` parameter in log calls, and consistently including relevant metadata like user_id, request_id, and operation context in your log entries.

**How can I prevent sensitive data from appearing in logs?**

Prevent sensitive data in logs by implementing custom filters that automatically redact patterns like credit cards, passwords, and API keys. Use regular expressions to identify and replace sensitive information with placeholder text like `***REDACTED***`.