---
date: 2025-07-21
id: vercel-ai-sdk-monitoring-with-opentelemetry
title: Integrating Vercel AI SDK Telemetry with SigNoz
description: Enable telemetry in your Vercel AI application with OpenTelemetry and send traces, logs, and metrics to SigNoz
---

## Overview

This guide walks you through enabling telemetry in the Vercel AI SDK and streaming that data to SigNoz Cloud using OpenTelemetry. By the end of this setup, you'll be able to monitor AI-related function calls like generateText() or generateStream()—including request durations, inputs, outputs, and custom metadata—directly within SigNoz.

Instrumenting your AI workflows with telemetry helps you observe performance, debug latency, and trace AI interactions end-to-end across your Next.js application. This is especially valuable when building production-grade AI features where visibility into model behavior and user interactions is critical.

You can explore our example Vercel AI SDK chatbot, which includes monitoring via OpenTelemetry, [here](https://github.com/SigNoz/vercel-ai-sdk-opentelemetry-example).

## Prerequisites

- Next.js app
- Vercel AI SDK integrated into the app
- [SigNoz Cloud Account](https://signoz.io/teams/)
- SigNoz Ingestion Key

## Instrument your Next.js application

Check out detailed instructions on how to set up OpenTelemetry instrumentation in your Nextjs applications and view your application traces in SigNoz over [here](https://signoz.io/docs/instrumentation/opentelemetry-nextjs/).

## Send traces directly to SigNoz Cloud

**Step 1.** Install OpenTelemetry packages

```bash
npm install @vercel/otel @opentelemetry/api
```

**Step 2.** Update **`next.config.mjs`** to include instrumentationHook

> This step is only needed when using NextJs 14 and below
> 

```jsx
/** @type {import('next').NextConfig} */
const nextConfig = {
	// include instrumentationHook experimental feature
	experimental: {
		instrumentationHook: true,
	},
};
export default nextConfig;
```

**Step 3.** Create **`instrumentation.ts`** file(in root project directory)

```jsx
import { registerOTel, OTLPHttpJsonTraceExporter } from '@vercel/otel';
// Add otel logging
import { diag, DiagConsoleLogger, DiagLogLevel } from '@opentelemetry/api';
diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.ERROR); // set diaglog level to DEBUG when debugging
export function register() {
  registerOTel({
    serviceName: '<service_name>',
    traceExporter: new OTLPHttpJsonTraceExporter({
        url: 'https://ingest.<region>.signoz.cloud:443/v1/traces',
        headers: { 'signoz-ingestion-key': '<your-ingestion-key>' },
    }),
  });
}

```

- **`<service_name>`** is the name of your service
- Set the **`<region>`** to match your SigNoz Cloud [**region**](https://signoz.io/docs/ingestion/signoz-cloud/overview/#endpoint)
- Replace **`<your-ingestion-key>`** with your SigNoz [**ingestion key**](https://signoz.io/docs/ingestion/signoz-cloud/keys/)

> The instrumentation file should be in the root of your project and not inside the app or pages directory. If you're using the src folder, then place the file inside src alongside pages and app.
> 

Your Next.js app should be properly instrumented now.

**Step 4.** Verify Instrumentation Locally

Run your Next.js application in development mode:

```bash
npm run dev
```

In your terminal, you should see output from OpenTelemetry when the application starts, confirming that the instrumentation.ts file was loaded correctly. 

<figure data-zoomable align='center'>
    <img src="/img/docs/llm/vercel/vercel-run-output.webp" alt="Terminal output showing OpenTelemetry initialization"/>
    <figcaption><i>Output showing OpenTelemetry initialization when running your application</i></figcaption>
</figure>

To see detailed trace export logs, you can temporarily set the DiagLogLevel to DEBUG in instrumentation.ts:

```jsx
diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);
```

## Enable Telemetry for Vercel AI SDK

The Vercel AI SDK uses [OpenTelemetry](https://signoz.io/blog/what-is-opentelemetry/) to collect telemetry data. OpenTelemetry is an open-source observability framework designed to provide standardized instrumentation for collecting telemetry data.

## Enabling Telemetry

Check out more detailed information about Vercel AI SDK’s telemetry options visit [here](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#telemetry).

You can then use the `experimental_telemetry` option to enable telemetry on specific function calls while the feature is experimental:

```jsx
const result = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a short story about a cat.',
  experimental_telemetry: { isEnabled: true },
});
```

When telemetry is enabled, you can also control whether you want to record the input values and the output values for the function. By default, both are enabled. You can disable them by setting the `recordInputs` and `recordOutputs` options to `false`.

```jsx
experimental_telemetry: { isEnabled: true, recordInputs: false, recordOutputs: false}
```

Disabling the recording of inputs and outputs can be useful for privacy, data transfer, and performance reasons. You might, for example, want to disable recording inputs if they contain sensitive information.

## Telemetry Metadata

You can provide a `functionId` to identify the function that the telemetry data is for, and `metadata` to include additional information in the telemetry data.

```jsx
const result = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a short story about a cat.',
  experimental_telemetry: {
    isEnabled: true,
    functionId: 'my-awesome-function',
    metadata: {
      something: 'custom',
      someOtherThing: 'other-value',
    },
  },
});
```

## Custom Tracer

You may provide a `tracer` which must return an OpenTelemetry `Tracer`. This is useful in situations where you want your traces to use a `TracerProvider` other than the one provided by the `@opentelemetry/api` singleton.

```jsx
const tracerProvider = new NodeTracerProvider();
const result = await generateText({
  model: openai('gpt-4-turbo'),
  prompt: 'Write a short story about a cat.',
  experimental_telemetry: {
    isEnabled: true,
    tracer: tracerProvider.getTracer('ai'),
  },
});
```

Your Vercel AI SDK commands should now automatically emit traces, spans, and events. You can find more details on the types of spans and events generated [here](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#collected-data). 

Finally, you should be able to view this data in Signoz Cloud under the traces tab:

<figure data-zoomable align='center'>
    <img src="/img/docs/llm/vercel/vercel-traces-view.webp" alt="Traces View"/>
    <figcaption><i>Traces of your Vercel AI Application</i></figcaption>
</figure>

If you click on any span, you can see the detailed trace of which it is a part of.

<figure data-zoomable align='center'>
    <img src="/img/docs/llm/vercel/vercel-traces-detailed-view.webp" alt="Detailed Traces View"/>
    <figcaption><i>Detailed Trace view of your Vercel AI Application</i></figcaption>
</figure>


## Dashboard

You can also check out our custom Vercel AI SDK dashboard [here](https://signoz.io/docs/dashboards/dashboard-templates/vercel-ai-sdk-dashboard/) which provides specialized visualizations for monitoring your AI applications. The dashboard includes pre-built charts and metrics specifically tailored for Vercel AI SDK telemetry data, along with import instructions to get started quickly.

<figure data-zoomable align='center'>
    <img src="/img/docs/dashboards/dashboard-templates/vercel-ai-sdk-dashboard.webp" alt="Vercel AI SDK Dashboard"/>
    <figcaption><i>Vercel AI SDK Dashboard</i></figcaption>
</figure>
